{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eae6406",
   "metadata": {},
   "source": [
    "! Run below 2 lines after converting code block to install dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d59ac12e",
   "metadata": {},
   "source": [
    "%pip install ebooklib lxml six boto3 spacy pandas google-cloud-texttospeech\n",
    "!python -m spacy download pl_core_news_sm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53835e2b",
   "metadata": {},
   "source": [
    "%pip install pydub mutagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fe7743-6e59-4a3b-ace4-418da0879015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import boto3\n",
    "import random\n",
    "import spacy\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from ebooklib import epub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4154c07-0cb0-432a-aed3-f7ee998f826c",
   "metadata": {},
   "source": [
    "### TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a96d289-53b4-4187-913a-c9fe2bf13aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa37ae61-bc71-4ab5-81b8-9961b0d187ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../your-project-creds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8442a497-e67d-4f99-bfac-5dac0f199e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial voice lists\n",
    "INITIAL_FEMALE_VOICES = [\n",
    "    ['pl-PL-Wavenet-D', 0], ['pl-PL-Wavenet-E', 0], \n",
    "    ['pl-PL-Wavenet-D', 2], ['pl-PL-Wavenet-E', 2], ['pl-PL-Wavenet-A', 2],\n",
    "    ['pl-PL-Wavenet-D', -2], ['pl-PL-Wavenet-E', -2], ['pl-PL-Wavenet-A', -2],\n",
    "    ['pl-PL-Wavenet-D', 4], ['pl-PL-Wavenet-E', 4], ['pl-PL-Wavenet-A', 4],\n",
    "    ['pl-PL-Wavenet-D', -4], ['pl-PL-Wavenet-E', -4], ['pl-PL-Wavenet-A', -4]]\n",
    "INITIAL_MALE_VOICES = [\n",
    "    ['pl-PL-Wavenet-B', 0], ['pl-PL-Wavenet-C', 0],\n",
    "    ['pl-PL-Wavenet-B', 2], ['pl-PL-Wavenet-C', 2],\n",
    "    ['pl-PL-Wavenet-B', -2], ['pl-PL-Wavenet-C', -2],\n",
    "    ['pl-PL-Wavenet-B', 4], ['pl-PL-Wavenet-C', 4],\n",
    "    ['pl-PL-Wavenet-B', -4], ['pl-PL-Wavenet-C', -4]\n",
    "]\n",
    "\n",
    "# Store voice assignments for names\n",
    "voice_assignments = {}\n",
    "default_voice = ['pl-PL-Wavenet-A', 0]\n",
    "gtts_char_limit = 5000\n",
    "\n",
    "def split_title(title):\n",
    "    \"\"\"Split title into main title and subtitle if ':' exists.\"\"\"\n",
    "    if ':' in title:\n",
    "        main_title, subtitle = title.split(':', 1)\n",
    "        return f'<emphasis level=\"strong\">{main_title.strip()}</emphasis><break time=\"250ms\"/><emphasis level=\"moderate\">{subtitle.strip()}</emphasis><break time=\"500ms\"/>'\n",
    "    else:\n",
    "        return f'<emphasis level=\"strong\">{title}</emphasis><break time=\"500ms\"/>'\n",
    "\n",
    "def process_paragraph(paragraph, female_voices, male_voices):\n",
    "    \"\"\"Process a paragraph based on whether it starts with a name.\"\"\"\n",
    "    words = paragraph.split(':')\n",
    "    if len(words) > 1 and len(words[0].split(\" \")) < 3:\n",
    "        name = words[0]\n",
    "        rest_of_paragraph = ':'.join(words[1:])\n",
    "        \n",
    "        # Assign voice based on gender and ensure consistency\n",
    "        if name not in voice_assignments:\n",
    "            if name[-1] == 'a' and female_voices:\n",
    "                voice_assignments[name] = female_voices.pop(0)\n",
    "            elif male_voices:\n",
    "                voice_assignments[name] = male_voices.pop(0)\n",
    "        return rest_of_paragraph, voice_assignments[name]\n",
    "    else:\n",
    "        return paragraph, default_voice\n",
    "\n",
    "def add_ssml(story):\n",
    "    \"\"\"Add SSML tags to a story.\"\"\"\n",
    "    # Reset voice lists for each story\n",
    "    female_voices = INITIAL_FEMALE_VOICES.copy()\n",
    "    male_voices = INITIAL_MALE_VOICES.copy()\n",
    "\n",
    "    paragraphs = [\n",
    "        {\n",
    "            \"ssml\": split_title(story['title']),\n",
    "            \"voice\": default_voice\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for p in story['paragraphs']:\n",
    "        pp, voice = process_paragraph(p, female_voices, male_voices)\n",
    "        if paragraphs[-1][\"voice\"] == voice and (len(paragraphs[-1][\"ssml\"]) + len(pp)) < gtts_char_limit:\n",
    "            paragraphs[-1][\"ssml\"] = paragraphs[-1][\"ssml\"] + '<break time=\"500ms\"/>' + pp\n",
    "        else:\n",
    "            paragraphs.append({\"ssml\": pp, \"voice\": voice})\n",
    "            \n",
    "    # Reset voice assignments for the next story\n",
    "    voice_assignments.clear()\n",
    "    for p in paragraphs:\n",
    "        p[\"ssml\"] = f\"<speak>{p['ssml']}</speak>\"\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ff69f4-0742-4d8d-8569-135f65c966ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_mp3s(file_list, output_file):\n",
    "    \"\"\"\n",
    "    Concatenate multiple MP3 files into a single file using ffmpeg.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_list: List of MP3 file paths to concatenate.\n",
    "    - output_file: Path of the output MP3 file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the input string for ffmpeg\n",
    "    input_str = \"|\".join(file_list)\n",
    "    \n",
    "    # Form the ffmpeg command\n",
    "    cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', f'concat:{input_str}',\n",
    "        '-acodec', 'copy',\n",
    "        output_file\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    subprocess.run(cmd)\n",
    "\n",
    "# Example usage:\n",
    "# concatenate_mp3s([\"file1.mp3\", \"file2.mp3\", \"file3.mp3\"], \"output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6886a309-d861-4c3c-bb1c-9d2e8be44f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(full_text):\n",
    "    # Split the text into paragraphs to avoid breaking sentences\n",
    "    paragraphs = full_text.split(\"\\n\")\n",
    "    \n",
    "    # AWS Polly's limit for neural voices is 6000 characters\n",
    "    CHUNK_SIZE = 3000\n",
    "    chunk_texts = []\n",
    "    temp_chunk = \"\"\n",
    "\n",
    "    # Organize paragraphs to fit within the character limit without breaking them\n",
    "    for paragraph in paragraphs:\n",
    "        if len(temp_chunk) + len(paragraph) < CHUNK_SIZE:\n",
    "            temp_chunk += paragraph + \"\\n\"\n",
    "        else:\n",
    "            chunk_texts.append(temp_chunk.strip())\n",
    "            temp_chunk = paragraph + \"\\n\"\n",
    "\n",
    "    # Add any remaining text\n",
    "    if temp_chunk:\n",
    "        chunk_texts.append(temp_chunk.strip())\n",
    "    return chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70962ecb-5b9e-4e6a-a31c-5fb5528104c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_with_polly(story, output_path):\n",
    "    text = story['title'] + \"\\n\\n\" + \"\\n\".join(story['paragraphs'])\n",
    "    \n",
    "    # Split the text into chunks and store them\n",
    "    chunk_texts = split_text_into_chunks(text)\n",
    "    \n",
    "    # Temporary file paths will be stored here\n",
    "    temp_files = []\n",
    "    client = boto3.client('polly')\n",
    "\n",
    "    for chunk in chunk_texts:\n",
    "        response = client.synthesize_speech(Text=chunk, OutputFormat='mp3', VoiceId='Ewa')\n",
    "        if \"AudioStream\" in response:\n",
    "            # Save each chunk to a temporary file\n",
    "            temp_fd, temp_filename = tempfile.mkstemp(suffix=\".mp3\")\n",
    "            with os.fdopen(temp_fd, 'wb') as tmpf:\n",
    "                tmpf.write(response['AudioStream'].read())\n",
    "            temp_files.append(temp_filename)\n",
    "    \n",
    "    # Ensure the 'audio' directory exists\n",
    "    audio_folder = os.path.dirname(output_path)\n",
    "    if not os.path.exists(audio_folder):\n",
    "        os.makedirs(audio_folder)\n",
    "    \n",
    "    # Use ffmpeg to concatenate the audio files\n",
    "    concatenate_mp3s(temp_files, output_path)\n",
    "    \n",
    "    # Clean up the temporary files\n",
    "    for temp_file in temp_files:\n",
    "        os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776dd9f7-8a4d-418a-9e62-0a8f81118f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_tts(voice_param, ssml_input, language, output):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(ssml=ssml_input)\n",
    "\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=language,\n",
    "        name=voice_param[0]  # You can pick different voices here\n",
    "    )\n",
    "\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3,\n",
    "        speaking_rate=0.9,  # Set the speed here\n",
    "        pitch=voice_param[1]  # Increase pitch by 2 semitones\n",
    "    )\n",
    "\n",
    "    response = client.synthesize_speech(\n",
    "        input=input_text,\n",
    "        voice=voice,\n",
    "        audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    with open(output, \"wb\") as out:\n",
    "        out.write(response.audio_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fac582-1c0e-4fdd-8c43-403952ed387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_with_google_tts(story, output_path):    \n",
    "    # Split the text into chunks and store them\n",
    "    chunk_texts = add_ssml(story)\n",
    "    \n",
    "    # Temporary file paths will be stored here\n",
    "    temp_files = []\n",
    "    client = boto3.client('polly')\n",
    "\n",
    "    for chunk in chunk_texts:\n",
    "        # Save each chunk to a temporary file\n",
    "        temp_fd, temp_filename = tempfile.mkstemp(suffix=\".mp3\")\n",
    "        temp_files.append(temp_filename)\n",
    "        google_tts(chunk[\"voice\"], chunk[\"ssml\"], \"pl-PL\", temp_filename)\n",
    "    \n",
    "    # Ensure the 'audio' directory exists\n",
    "    audio_folder = os.path.dirname(output_path)\n",
    "    if not os.path.exists(audio_folder):\n",
    "        os.makedirs(audio_folder)\n",
    "    \n",
    "    # Use ffmpeg to concatenate the audio files\n",
    "    concatenate_mp3s(temp_files, output_path)\n",
    "    \n",
    "    # Clean up the temporary files\n",
    "    for temp_file in temp_files:\n",
    "        try:\n",
    "            os.remove(temp_file)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a4069-f89d-4cc4-aaf8-cf7a8690ad79",
   "metadata": {},
   "source": [
    "# Audiobook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba7e0d0-6974-49f6-83bc-306f5441f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from mutagen.m4a import M4A\n",
    "from mutagen.mp4 import MP4, MP4Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b75a28-dd9e-463b-9fc0-412336ef17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0597a389-30f8-465c-9e30-d3c64a3b0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_checksum(file_path):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        # Read only 4K at a time to avoid running out of memory\n",
    "        for byte_block in iter(lambda: f.read(4096),b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def generate_checksum(mp3_file, chapter_titles, book_title, book_cover, book_text):\n",
    "    mp3_checksum = generate_file_checksum(mp3_file)\n",
    "    checksum_str = mp3_checksum + str(chapter_titles) + book_title + book_cover + book_text\n",
    "    return hashlib.sha256(checksum_str.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fcfe334-5ace-411f-862a-c2f2fbcf83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_title(title):\n",
    "    main_title = title.split(\":\")[0]  # Keep only text before ':'\n",
    "    sanitized_title = re.sub('[^\\w\\s-]', '', main_title)  # Remove illegal characters\n",
    "    return sanitized_title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9fb056e-7cfc-4a81-8418-5345737d7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_apple_audiobooks(mp3_files, chapter_titles, book_title, book_cover, book_text, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    version = 1 # change when the logic changes\n",
    "    if len(mp3_files) != len(chapter_titles):\n",
    "        print(\"Warning: The number of mp3_files doesn't match the number of chapter_titles.\")\n",
    "        return\n",
    "\n",
    "    checksum_filename = f'{output_folder}/checksums.json'\n",
    "    try:\n",
    "        with open(checksum_filename, 'r') as f:\n",
    "            old_checksums = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        old_checksums = {}\n",
    "\n",
    "    new_checksums = {}\n",
    "\n",
    "    for ind, mp3_file in enumerate(mp3_files):\n",
    "        chapter_title = chapter_titles[ind]\n",
    "        sanitized_title = sanitize_title(chapter_title)\n",
    "        new_checksum = generate_checksum(mp3_file, sanitized_title, f\"{book_title}-v{version}\", book_cover, book_text)\n",
    "        new_checksums[sanitized_title] = new_checksum\n",
    "\n",
    "        if sanitized_title in old_checksums and new_checksum == old_checksums[sanitized_title]:\n",
    "            continue\n",
    "\n",
    "        audio = AudioSegment.from_mp3(mp3_file)\n",
    "        m4b_filename = f'{output_folder}/{sanitized_title}.m4b'\n",
    "        audio.export(m4b_filename, format=\"ipod\")\n",
    "\n",
    "        metadata = MP4(m4b_filename)\n",
    "        metadata[\"\\xa9nam\"] = chapter_title\n",
    "        metadata[\"\\xa9aut\"] = \"Fatih Kurt\"  # Author\n",
    "        metadata[\"\\xa9alb\"] = book_title  # Album\n",
    "        metadata[\"\\xa9gen\"] = \"Audiobook\"  # Genre\n",
    "        metadata[\"aART\"] = \"Fatih Kurt\"  # Album Artist\n",
    "        metadata[\"\\xa9day\"] = \"2023\"  # Year\n",
    "        metadata[\"\\xa9wrt\"] = \"Fatih Kurt\"  # Writer\n",
    "        metadata[\"\\xa9grp\"] = book_title  # Content Group\n",
    "        metadata[\"covr\"] = [MP4Cover(open(book_cover, \"rb\").read(), imageformat=MP4Cover.FORMAT_JPEG)]  # Cover art\n",
    "        metadata[\"cpil\"] = 1  # Compilation (True or False)\n",
    "        metadata[\"pgap\"] = 1  # Gapless playback (True or False)\n",
    "        metadata[\"trkn\"] = [(ind+1, len(mp3_files))]  # Track number and total tracks\n",
    "\n",
    "        metadata.save()\n",
    "        print(f\"Successfully generated {sanitized_title}.m4b\")\n",
    "\n",
    "    # Save new checksums\n",
    "    with open(checksum_filename, 'w') as f:\n",
    "        json.dump(new_checksums, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fcd5c-ff63-4153-ae91-30a9e85d2b2d",
   "metadata": {},
   "source": [
    "# Ebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082940d8-9139-440d-80e7-14d5ea1d0b1c",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f165e6d1-9dd9-4122-a8ef-3b13c65f231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRootFinder:\n",
    "    def __init__(self, cache_file=\"root_cache.json\"):\n",
    "        self.nlp = spacy.load(\"pl_core_news_sm\")\n",
    "        self.root_cache = {}\n",
    "        self.cache_counter = 0\n",
    "        self.cache_file = cache_file\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, \"r\", encoding='utf-8') as f:\n",
    "                self.root_cache = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.root_cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        with open(self.cache_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(self.root_cache, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def root(self, word):\n",
    "        if word in self.root_cache:\n",
    "            return self.root_cache[word]\n",
    "\n",
    "        doc = self.nlp(word)\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_\n",
    "            ll = lemma.split(\" \")\n",
    "            if len(ll) > 1 and (ll[1] == \"być\" or ll[1] == \"by\"):\n",
    "                root = ll[0]\n",
    "            elif len(ll) > 1:\n",
    "                print(f\"Unknown combined word: {word} / {lemma}\")\n",
    "                root = lemma\n",
    "            else:\n",
    "                root = lemma\n",
    "\n",
    "            self.root_cache[word] = root\n",
    "            self.cache_counter += 1\n",
    "\n",
    "            if self.cache_counter >= 10:\n",
    "                self.save_cache()\n",
    "                self.cache_counter = 0\n",
    "\n",
    "            return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01007054-d3ba-4a7e-b494-d8a0e756ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_root_finder = WordRootFinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e3a33-41b7-4422-8d9c-82e640869f03",
   "metadata": {},
   "source": [
    "## Book Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95cbebd7-7f5e-4c35-aa6b-b36a20847dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl50k = {}\n",
    "with open(\"../pl_50k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for l in f.readlines():\n",
    "        ps = l.split()\n",
    "        pl50k[ps[0]] = {\"rank\": len(pl50k) + 1, \"count\": int(ps[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73e1143f-64a4-4834-9ced-55c9d35f0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(filename, break_chapters, max_story_length=2700):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    stories = content.split('\\n\\n\\n')\n",
    "    for story in stories:\n",
    "        is_split, part = False, 0\n",
    "        total_chars = 0\n",
    "        paragraphs, words = [], []\n",
    "        lines = story.strip().split('\\n\\n')\n",
    "        if len(lines) >= 2 and lines[-2] == \"Słownictwo:\":\n",
    "            word_margin = 2\n",
    "            words = [lines[-2]] + lines[-1].strip().split('\\n')\n",
    "        else:\n",
    "            word_margin = 0\n",
    "            words = []\n",
    "\n",
    "        title = lines[0]\n",
    "        i = 1\n",
    "        while i < len(lines)-word_margin:\n",
    "            line = lines[i]\n",
    "            if line == '':\n",
    "                i += 1\n",
    "            elif line[0].isalpha() or line[0].isdigit() or True:\n",
    "                i += 1\n",
    "                if break_chapters and total_chars > (max_story_length / 2) and len(line) > (max_story_length - total_chars) and len(paragraphs) > 0:\n",
    "                    is_split = True\n",
    "                    part += 1\n",
    "                    stitle = f\"Część {part}\"\n",
    "                    partp = []\n",
    "                    if part == 1:\n",
    "                        stitle = f\"{title}\"\n",
    "                        partp = [f\"Część {part}\"]\n",
    "                    yield {\n",
    "                        'title': stitle,\n",
    "                        'paragraphs': partp + paragraphs,\n",
    "                        'words': []\n",
    "                    }\n",
    "                    total_chars = 0\n",
    "                    paragraphs = []\n",
    "                else:\n",
    "                    total_chars += len(line)\n",
    "                    paragraphs.append(line)\n",
    "            else:\n",
    "                print(f\"unexpected entry: {line}\")\n",
    "        if len(paragraphs) > 0:\n",
    "            stitle = title\n",
    "            if is_split:\n",
    "                stitle = f\"Część {part+1}\"\n",
    "            yield {\n",
    "                'title': stitle,\n",
    "                'paragraphs': paragraphs,\n",
    "                'words': words\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33c90f43-4230-425b-a23b-51f3841a603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary():\n",
    "    known_words = {}\n",
    "    with open(\"../known_words.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        known_words = json.load(f)\n",
    "    samples = pd.read_csv(\"../samples.csv\", delimiter=\";\")\n",
    "    res = {}\n",
    "    for _, row in samples.iterrows():\n",
    "        if row[\"Word\"].lower() not in known_words:\n",
    "            res[row[\"Word\"].lower()] = row[\"Translation\"]\n",
    "    return known_words, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "457ad5de-9854-4005-af6d-36d3d8107bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encapsulate_chapter(title, chapter_content):\n",
    "    return f'''<html>\n",
    "    <head>\n",
    "        <title>{title}</title>\n",
    "        <link href=\"\"style/book-style.css\" rel=\"stylesheet\" type=\"text/css\" />\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>{title}</h1>\n",
    "        {chapter_content}\n",
    "    </body>\n",
    "    </html>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db9f049-0a98-4d52-ada9-1ddf686e5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_html(word, correct_translation, incorrect_answers, chapterid):\n",
    "    all_options = [correct_translation] + incorrect_answers\n",
    "    random.shuffle(all_options)\n",
    "    title = word\n",
    "    root = word_root_finder.root(word)\n",
    "    if root != word:\n",
    "        title = combine_root_with_word(word, root)\n",
    "    question_html = f'''\n",
    "    <fieldset class=\"noBreak\">\n",
    "        <legend><b>{title}</b></legend>\n",
    "        <div class=\"options\">\n",
    "    '''\n",
    "    for idx, option in enumerate(all_options):\n",
    "        qid = f\"label_{chapterid}_{word}_{idx}\"\n",
    "        question_html += f'''\n",
    "            <div class=\"question\" id=\"{qid}\" onclick=\"checkAnswer('{word}', '{correct_translation}', '{qid}', '{chapterid}')\">\n",
    "                <label>{option}</label>\n",
    "            </div>\n",
    "        '''\n",
    "    question_html += '</div></fieldset></br>'\n",
    "    return question_html\n",
    "\n",
    "def combine_root_with_word(word, root):\n",
    "    if word == root:\n",
    "        return word\n",
    "    \n",
    "    common_length = 0\n",
    "    \n",
    "    for i in range(min(len(word), len(root))):\n",
    "        if word[i] == root[i]:\n",
    "            common_length += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    common = root[:common_length]\n",
    "    unique_ending1 = word[common_length:]\n",
    "    unique_ending2 = root[common_length:]\n",
    "    if len(unique_ending1) == 0:\n",
    "        return f\"{common}({unique_ending2})\"\n",
    "    if len(unique_ending2) == 0:\n",
    "        return f\"{common}({unique_ending1})\"\n",
    "\n",
    "    return f\"{common}({unique_ending2}/{unique_ending1})\"\n",
    "\n",
    "\n",
    "def generate_incorrect_answers(word, correct_translation, translations):\n",
    "    options = [v for k, v in translations if k != word and correct_translation != v]\n",
    "    if len(options) < 4:\n",
    "        return options\n",
    "    return random.sample(options, 3)\n",
    "\n",
    "\n",
    "def add_translation_test(translations, chapterid):\n",
    "    questions_html = f'<h2>Test słownictwa (Słowa: {len(translations)})</h2>'\n",
    "    for word, correct_translation in translations:\n",
    "        incorrect_answers = generate_incorrect_answers(word, correct_translation, translations)\n",
    "        questions_html += generate_question_html(word, correct_translation, incorrect_answers, chapterid)\n",
    "    return questions_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42244e6-08c6-4f8c-bb4c-5e4692e279cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class book_handler:\n",
    "    def __init__(self, title, book_dir, poster, break_chapters=False):\n",
    "        self.tts = True # enable tts generation\n",
    "        self.title = title\n",
    "        self.book_dir = book_dir\n",
    "        self.poster = poster\n",
    "        self.stories = []\n",
    "        self.foot_notes = []\n",
    "        self.better_translate = {}\n",
    "        self.stories = list(parse_stories(os.path.join(book_dir, \"book.txt\"), break_chapters))\n",
    "        self.translated_word_ctr = Counter()\n",
    "\n",
    "    def add_tooltips(self, paragraph, dictionary, known_words, used = set()):\n",
    "        # dictonary does not have known words\n",
    "        words = re.findall(r'\\b[^\\W_]+\\b', paragraph, flags=re.UNICODE)\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word[0] == \"<\" or word[-1] == \">\":\n",
    "                continue\n",
    "            wl = self.remove_non_chars(word)\n",
    "            wl_root = word_root_finder.root(wl.lower())\n",
    "            if wl_root is None:\n",
    "                wl_root = wl.lower()\n",
    "            if wl_root in dictionary and wl.lower() in dictionary and wl_root != dictionary[wl_root].lower():\n",
    "                used.add(wl.lower())\n",
    "                self.translated_word_ctr[wl.lower()] += 1\n",
    "                html = f\"\"\"<span class='non-breaking'><span class='overlay-text' data-translation='{dictionary[wl.lower()]}'></span>{word}</span>\"\"\"\n",
    "                words[i] = html\n",
    "            if wl_root not in known_words and wl_root not in dictionary and len(wl_root) > 0 and wl_root not in self.better_translate:\n",
    "                self.better_translate[wl_root] = word\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def remove_non_chars(self, text):\n",
    "        # Define a pattern to match non-characters\n",
    "        pattern = r'[^a-zA-ZąćęłńóśźżĄĆĘŁŃÓŚŹŻ\\s]'\n",
    "    \n",
    "        # Remove non-characters using regular expressions\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "        return cleaned_text.lower()\n",
    "\n",
    "    def process_story(self, ind, story, known_words, dictionary):\n",
    "        start_ind = len(self.foot_notes)\n",
    "        used = set()\n",
    "        title = self.add_tooltips(story[\"title\"], dictionary, known_words, used)\n",
    "        paragraphs = ''\n",
    "        for paragraph in story['paragraphs']:\n",
    "            p_tooltip = self.add_tooltips(paragraph, dictionary, known_words, used)\n",
    "            paragraphs += f'<p><button class=\"audioControl paused\" onclick=\"toggleAudio()\"></button>{p_tooltip}</p> ' # use \\t for tab character\n",
    "        top_words = self.top_words_to_learn()\n",
    "        sublist = []\n",
    "        for w, c in top_words:\n",
    "            if w in used:\n",
    "                sublist.append([w, dictionary[self.remove_non_chars(w.lower())]])\n",
    "        test = add_translation_test(sublist, f\"chapter{ind}\")\n",
    "        words = ''\n",
    "        fns = f'<section epub:type=\"footnotes\" id=\"ch{ind}footnotes\"><hr /><ol class=\"footnotes\">'\n",
    "        for fn in self.foot_notes[start_ind:]:\n",
    "            fns += f'<li epub:type=\"footnote\" id=\"fn{fn[\"id\"]}\">{fn[\"def\"]}</li>'\n",
    "        fns += '</ol></section>'\n",
    "        return encapsulate_chapter(title, f'{paragraphs}{test}{fns}{words}') # add page break after each story\n",
    "    \n",
    "    def create_epub(self):\n",
    "        book = self.setup_book_metadata()\n",
    "        self.add_cover_image(book)\n",
    "        self.add_media_files(book)\n",
    "        mp3_files, chapter_titles = self.add_chapters_to_book(book)\n",
    "        self.write_epub_file(book)\n",
    "        cover = os.path.join(self.book_dir, self.poster)\n",
    "        data = open(os.path.join(self.book_dir, \"book.txt\"), 'r', encoding=\"utf-8\").read()\n",
    "        output = os.path.join(self.book_dir, \"audio-book\")\n",
    "        create_apple_audiobooks(mp3_files, chapter_titles, self.title, cover, data, output)\n",
    "        self.report()\n",
    "\n",
    "    def setup_book_metadata(self):\n",
    "        book = epub.EpubBook()\n",
    "        book.set_identifier('sample_id')\n",
    "        book.set_title(self.title)\n",
    "        book.set_language('pl')\n",
    "        book.add_author(\"Fatih Kurt\")\n",
    "        return book\n",
    "\n",
    "    def add_cover_image(self, book):\n",
    "        book.set_cover(self.poster, open(os.path.join(self.book_dir, self.poster), 'rb').read())\n",
    "\n",
    "    def add_media_files(self, book):\n",
    "        media_dir = os.path.join(self.book_dir, \"images\")\n",
    "        self.add_media_from_dir(book, media_dir)\n",
    "        self.add_media_from_dir(book, \"book-images\")\n",
    "\n",
    "    def add_media_from_dir(self, book, media_dir):\n",
    "        if media_dir and os.path.isdir(media_dir):\n",
    "            for media_file in os.listdir(media_dir):\n",
    "                if media_file.endswith(('jpg', 'jpeg', 'png', 'gif', 'svg')):\n",
    "                    book.add_item(epub.EpubImage(\n",
    "                        file_name=media_file,\n",
    "                        content=open(os.path.join(media_dir, media_file), 'rb').read()\n",
    "                    ))\n",
    "\n",
    "    def add_chapters_to_book(self, book):\n",
    "        style = open(\"book-style.css\", \"r\").read()\n",
    "        nav_css = epub.EpubItem(uid=\"style_nav\", file_name=\"style/book-style.css\", media_type=\"text/css\", content=style)\n",
    "        script = open(\"book-script.js\", \"r\").read()\n",
    "        nav_script = epub.EpubItem(uid=\"script_nav\", file_name=\"script/book-script.js\", media_type=\"application/javascript\", content=script)\n",
    "        chapter_objs = []\n",
    "        mp3_files, chapter_titles = [], []\n",
    "        known_words, dictionary = get_dictionary()\n",
    "        for ind, story in enumerate(self.stories):\n",
    "            chapter_titles.append(story['title'])\n",
    "            clean_title = re.sub(r'[^a-zA-Z0-9]', '_', story['title'])\n",
    "            c = epub.EpubHtml(title=story['title'], file_name=f\"{clean_title}-{ind}.xhtml\", lang='pl')\n",
    "    \n",
    "            # Check for audio file and add it if it exists\n",
    "            audio_path = os.path.join(self.book_dir, \"audio\", f\"{ind+1}.mp3\")\n",
    "            if not os.path.exists(audio_path):\n",
    "                # Generate audio if it doesn't exist using AWS Polly\n",
    "                if self.tts:\n",
    "                    generate_audio_with_google_tts(story, audio_path)\n",
    "                else:\n",
    "                    s = \" \".join(story[\"paragraphs\"])\n",
    "                    print(f\"TTS disabled: {audio_path}: {s[:100]}...{s[-100:]}\")\n",
    "            audio_player = \"\"\n",
    "            if os.path.exists(audio_path):\n",
    "                mp3_files.append(audio_path)\n",
    "                audio_item = epub.EpubItem(\n",
    "                    uid=f\"audio_{ind+1}\",\n",
    "                    file_name=f\"audio/{ind+1}.mp3\",\n",
    "                    media_type=\"audio/mpeg\",\n",
    "                    content=open(audio_path, 'rb').read()\n",
    "                )\n",
    "                audio_player = f'<audio id=\"myAudio\" style=\"width: 100%;\" onplay=\"updateButtons()\" onpause=\"updateButtons()\" controls><source src=\"{audio_item.file_name}\" type=\"audio/mpeg\">Your browser does not support the audio tag.</audio>'\n",
    "                book.add_item(audio_item)\n",
    "                c.add_link(href=audio_item.file_name, rel=\"alternate\", type=audio_item.media_type, title=f\"Audio for {story['title']}\")\n",
    "            \n",
    "            c.content = audio_player + self.process_story(ind, story, known_words, dictionary)\n",
    "            c.add_link(href=nav_css.file_name, rel=\"stylesheet\", type=\"text/css\", title=\"CSS for Chapter\")\n",
    "            c.add_item(nav_script)\n",
    "            book.add_item(c)\n",
    "            chapter_objs.append(c)\n",
    "    \n",
    "        book.toc = chapter_objs\n",
    "        book.add_item(epub.EpubNcx())\n",
    "        book.add_item(epub.EpubNav())\n",
    "        book.add_item(nav_css)\n",
    "        book.add_item(nav_script)\n",
    "        book.spine = ['nav'] + chapter_objs\n",
    "        return mp3_files, chapter_titles\n",
    "\n",
    "    def write_epub_file(self, book):\n",
    "        epub.write_epub(os.path.join(self.book_dir, f\"{self.title}.epub\"), book, {})\n",
    "\n",
    "    def top_words_to_learn(self, percentile=100):\n",
    "        sorted_counts = sorted(self.translated_word_ctr.items(), key=lambda x: x[1], reverse=True)\n",
    "        s = 0\n",
    "        word_groups = {}\n",
    "        for k,v in sorted_counts:\n",
    "            s += v\n",
    "            wr = word_root_finder.root(k)\n",
    "            if wr not in word_groups:\n",
    "                word_groups[wr] = {\"w\":[k], \"t\": v}\n",
    "            else:\n",
    "                word_groups[wr][\"w\"].append(k)\n",
    "                word_groups[wr][\"t\"] += v\n",
    "        sorted_root_counts = sorted(word_groups.items(), key=lambda x: x[1][\"t\"], reverse=True)\n",
    "        res, ss = [], 0\n",
    "        for k,v in sorted_root_counts:\n",
    "            ss += v[\"t\"]\n",
    "            if k in v[\"w\"]:\n",
    "                res.append([k, v[\"t\"]])\n",
    "            else:\n",
    "                res.append([v[\"w\"][0], v[\"t\"]])\n",
    "            if percentile <= (100.0 * ss)/s:\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    def report(self):\n",
    "        print(f\"{len(self.better_translate)} words need translation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2ccd7",
   "metadata": {},
   "source": [
    "Disable TTS by setting `.tts = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3346efac-ce2a-4438-a66c-8a4219869431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 words need translation.\n"
     ]
    }
   ],
   "source": [
    "polish_short_stories = book_handler(\"Polskie opowiadania science-fiction\", \"polish-short-stories\", \"poster.png\")\n",
    "polish_short_stories.tts = False\n",
    "polish_short_stories.create_epub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eaad189-83a5-4723-8b02-ed8ef3223d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 words need translation.\n"
     ]
    }
   ],
   "source": [
    "polish_short_stories = book_handler(\"Polskie opowiadania science-fiction\", \"polish-short-stories\", \"poster.png\")\n",
    "polish_short_stories.create_epub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c86d68-853d-46d2-b2c7-2a470815962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 words need translation.\n"
     ]
    }
   ],
   "source": [
    "polish_short_conversations = book_handler(\"Polskie krótkie konwersacje dla B1\", \"short-conversations-polish-b1\", \"cover.jpg\")\n",
    "polish_short_conversations.create_epub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d80860de-249e-4e98-9ef8-6bd21631a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(polish_short_stories.better_translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "178d0876-70ce-433c-8a74-98e9fa5120ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(polish_short_conversations.better_translate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adc6fe",
   "metadata": {},
   "source": [
    "Use Below method to create a word test on MEMRISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4bb6669-d9a8-447e-b4ae-947e9ae4bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memrise_test(book, percentile=100):\n",
    "    known_words, dictionary = get_dictionary()\n",
    "    print(\"\\n\".join([f\"{k}\\t{dictionary[k]}\\t{v}\" for k,v in book.top_words_to_learn(percentile=percentile)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "840c2b9e-8a77-458e-ac14-f4e2cc124266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rozwijać\tto develop\t18\n",
      "związane\tbound\t11\n",
      "podejście\tapproach\t10\n",
      "wyzwanie\tchallenge\t9\n",
      "rozwój\tdevelopment\t9\n",
      "wyjątkowy\texceptional\t9\n",
      "wychowywania\tupbringing\t7\n",
      "korzyści\tbenefits\t6\n",
      "cenne\tvaluable\t6\n",
      "doceniać\tto appreciate\t6\n",
      "równowaga\tbalance\t6\n",
      "utrzymywać\tmaintain\t6\n",
      "spełnienia\tfulfillment\t6\n",
      "poglądami\tperspectives\t6\n",
      "zastanawiać\twonder\t6\n",
      "odnaleźć\tfind\t5\n",
      "dostosować\tadjust\t5\n",
      "przetrwać\tsurvive\t5\n",
      "poruszyć\ttouch\t5\n",
      "przesiadka\tchange\t5\n",
      "sąsiedztwo\tneighborhood\t5\n",
      "rodzicielstwo\tparenthood\t5\n",
      "narzeczony\tfiancé\t5\n",
      "współpraca\tcooperation\t5\n",
      "odnawiać\trenew\t4\n",
      "wpływ\tinfluence\t4\n",
      "oczekiwania\texpectations\t4\n",
      "podejmować\tundertake\t4\n",
      "niepokój\tworry\t4\n",
      "narzędzia\ttools\t4\n",
      "wychowanie\tupbringing\t4\n",
      "łączyła\tshe connected\t3\n",
      "zyskać\tgain\t3\n",
      "wychowawczymi\teducational\t3\n",
      "przyjemność\tpleasure\t3\n",
      "bezradna\thelpless\t3\n",
      "przeczytanej\tread\t3\n",
      "zastanowić\tto consider\t3\n",
      "nastawienia\tattitude\t3\n",
      "wyzwania\tchallenges\t3\n",
      "wytrzymałość\tendurance\t3\n",
      "rozstanie\tfarewell\t3\n",
      "wpłynąć\taffect\t3\n",
      "osobiście\tpersonally\t3\n",
      "wyznała\tconfessed\t3\n",
      "poprawę\timprovement\t3\n",
      "wzajemnego\tmutual\t3\n",
      "uroczystość\tceremony\t3\n",
      "zadowoliło\tsatisfied\t3\n",
      "pożegnali\tthey said goodbye\t3\n",
      "warzyw\tvegetables\t3\n",
      "współlokatorów\troommates\t3\n",
      "minęła\tpassed\t3\n",
      "połączyć\tto combine\t3\n",
      "uzależniona\tdependent\t3\n",
      "dołączyli\tthey joined\t3\n",
      "założyć\tto establish\t2\n",
      "kciuki\tthumbs\t2\n",
      "dostrzegać\tsee\t2\n",
      "zauważać\tnotice\t2\n",
      "przejść\tpass\t2\n",
      "uporać\tcope\t2\n",
      "samopoczucie\tfeeling\t2\n",
      "wychowawczą\teducational\t2\n",
      "osamotniona\tlonely\t2\n",
      "całkowicie\tentirely\t2\n",
      "rozwijanie\tdevelopment\t2\n",
      "borykam\tI struggle\t2\n",
      "narzeczonymi\tengaged couple\t2\n",
      "połączenie\tconnection\t2\n",
      "narzeczeni\tengaged couple\t2\n",
      "dzieleniu\tdividing\t2\n",
      "przyzwyczajona\tused to\t2\n",
      "ciągłe\tcontinuous\t2\n",
      "zarówno\tboth\t2\n",
      "zwłaszcza\tespecially\t2\n",
      "wymianę\texchange\t2\n",
      "postępy\tprogress\t2\n",
      "odwagi\tcourage\t2\n",
      "indziej\telsewhere\t2\n",
      "ławniczy\tjury\t2\n",
      "debaty\tdebate\t2\n",
      "wyrok\tsentence\t2\n",
      "pouczająca\teducational\t2\n",
      "wreszcie\tat last\t2\n",
      "powoli\tslowly\t2\n",
      "nawiązać\tconnect\t2\n",
      "zachwycona\tdelighted\t2\n",
      "pochodzisz\tyou come from\t2\n",
      "grozy\thorror\t2\n",
      "pożytecznego\tuseful\t2\n",
      "odnalezieniu\tfinding\t2\n",
      "charytatywnej\tcharitable\t2\n",
      "względów\treasons\t2\n",
      "odczuwać\tfeel\t2\n",
      "podjęłam\tI undertook\t2\n",
      "obowiązki\tresponsibilities\t2\n",
      "stosować\tapply\t2\n",
      "uczestnicy\tparticipants\t2\n"
     ]
    }
   ],
   "source": [
    "get_memrise_test(polish_short_conversations, percentile=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5439164-c0a5-426e-88e1-195932611309",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5bc9845-99ce-4cbb-8f49-72dbaebe42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aws_account_details():\n",
    "    sts_client = boto3.client('sts')\n",
    "    iam_client = boto3.client('iam')\n",
    "    \n",
    "    response = sts_client.get_caller_identity()\n",
    "    account_id = response['Account']\n",
    "\n",
    "    # Fetch account alias. This does not give the true email of the AWS account.\n",
    "    # It gives an alias that can be used to login via the AWS Management Console.\n",
    "    aliases = iam_client.list_account_aliases()['AccountAliases']\n",
    "    email = None\n",
    "\n",
    "    return account_id, aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8eab44",
   "metadata": {},
   "source": [
    "Test Google credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82ba15b4-4b71-4c5d-b104-a8d61228ee4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m account_id, aliases \u001b[38;5;241m=\u001b[39m \u001b[43mget_aws_account_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using AWS account ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m email:\n",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m, in \u001b[0;36mget_aws_account_details\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m sts_client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m iam_client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msts_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_caller_identity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m account_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Fetch account alias. This does not give the true email of the AWS account.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# It gives an alias that can be used to login via the AWS Management Console.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/client.py:963\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    959\u001b[0m     maybe_compress_request(\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m    961\u001b[0m     )\n\u001b[1;32m    962\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 963\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    969\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, operation_name\u001b[38;5;241m=\u001b[39moperation_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    974\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m    975\u001b[0m )\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/client.py:986\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 986\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    989\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    990\u001b[0m                 service_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m    995\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/endpoint.py:198\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    196\u001b[0m context \u001b[38;5;241m=\u001b[39m request_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[0;32m--> 198\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[1;32m    200\u001b[0m     request, operation_model, context\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    203\u001b[0m     attempts,\n\u001b[1;32m    204\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     exception,\n\u001b[1;32m    208\u001b[0m ):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/endpoint.py:134\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[1;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{op_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    132\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, op_name\u001b[38;5;241m=\u001b[39moperation_model\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maliased_event_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[0;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/signers.py:105\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/signers.py:189\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 189\u001b[0m \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/botocore/auth.py:418\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_auth\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoCredentialsError()\n\u001b[1;32m    419\u001b[0m     datetime_now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[1;32m    420\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime_now\u001b[38;5;241m.\u001b[39mstrftime(SIGV4_TIMESTAMP)\n",
      "\u001b[0;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "account_id, aliases = get_aws_account_details()\n",
    "print(f\"You are using AWS account ID: {account_id}\")\n",
    "if email:\n",
    "    print(f\"Account alias (aliases): {','.join(aliases)}\")\n",
    "else:\n",
    "    print(\"No account alias found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b7dac-dd0c-43f1-8c89-0de4a1bd5ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
